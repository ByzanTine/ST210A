#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Classic
\end_layout

\begin_layout Subsection
Statistical Decision Theory
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X=(X_{1},X_{2},...,X_{n})$
\end_inset

 be random vector.
 
\begin_inset Formula $X\sim P_{\theta}$
\end_inset

 distributed.
 
\begin_inset Formula $P_{\theta}\in\mathcal{P}=\left\{ P_{\theta}:\theta\in\Omega\right\} $
\end_inset

 a family of probability distribution.
 
\end_layout

\begin_layout Standard

\series bold
Parametric
\series default
 model: 
\begin_inset Formula $\theta\in\mathbb{R}^{P}.$
\end_inset


\end_layout

\begin_layout Standard
A 
\series bold
statistic
\series default
 
\begin_inset Formula $\delta(X)$
\end_inset

 is a function of the data 
\begin_inset Formula $X.$
\end_inset


\end_layout

\begin_layout Standard
In statistical estimation, the goal is to find a statistic 
\begin_inset Formula $\delta$
\end_inset

 such that 
\begin_inset Formula $\delta$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

close to
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $g(\theta).$
\end_inset

 
\begin_inset Formula $\delta$
\end_inset

 is called an 
\series bold
estimator
\series default
 of 
\begin_inset Formula $g(\theta)$
\end_inset


\end_layout

\begin_layout Standard

\series bold
Loss function
\series default
 
\begin_inset Formula $L(\theta,\delta(X))$
\end_inset

 is non-negative, with 
\begin_inset Formula $L(\theta,g(\theta))=0$
\end_inset


\end_layout

\begin_layout Standard

\series bold
Frequentist risk
\series default
 conditions on parameter: 
\begin_inset Formula 
\begin{alignat*}{1}
R(\theta,\delta)= & \mathbb{E}_{\theta}L(\theta,\delta(X))
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
An estimator 
\begin_inset Formula $\delta_{1}$
\end_inset

 is said to 
\series bold
dominate
\series default
 another estimator 
\begin_inset Formula $\delta_{2}$
\end_inset

 w.r.t a risk function 
\begin_inset Formula $R$
\end_inset

 iff 
\begin_inset Formula $R(\theta,\delta_{1})\le R(\theta,\delta_{2}),\forall\theta\in\Omega,$
\end_inset

 and the inequality is strict at at least one point.
\end_layout

\begin_layout Standard
An estimator 
\begin_inset Formula $\delta_{1}$
\end_inset

 is said to be 
\series bold
inadmissible
\series default
 w.r.t a risk function 
\begin_inset Formula $R$
\end_inset

 iff 
\begin_inset Formula $\exists\delta_{2}$
\end_inset

 estimator that dominates 
\begin_inset Formula $\delta_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
An estimator is called 
\series bold
unbiased
\series default
 iff 
\begin_inset Formula $\mathbb{E}_{\theta}\delta(X)=g(\theta),\forall\theta$
\end_inset

.
 
\end_layout

\begin_layout Standard
An estimator is called 
\series bold
equivariant
\series default
 w.r.t some transformation 
\begin_inset Formula $h$
\end_inset

 if after transforming data by function 
\begin_inset Formula $h,$
\end_inset

 then the corresponding estimator is also transformed by 
\begin_inset Formula $h.$
\end_inset

 MLE estimator is equivariant.
 
\end_layout

\begin_layout Standard
An estimator is 
\series bold
minimax
\series default
 iff it has the best 
\begin_inset Quotes eld
\end_inset

worst performance among all 
\begin_inset Formula $\theta$
\end_inset


\begin_inset Quotes erd
\end_inset

 among all estimator.
 
\end_layout

\begin_layout Standard

\series bold
Bayes risk
\series default
 assume 
\begin_inset Formula $\theta$
\end_inset

 is a random variable with density 
\begin_inset Formula $\pi$
\end_inset

 and integrate over 
\begin_inset Formula $\theta:$
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
R(\delta)= & \int R(\theta,\delta)\pi(\theta)d\theta
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
In general, 
\begin_inset Formula $\pi$
\end_inset

 does not have to be a probability density, but a general measure density.
 It is viewed as a 
\series bold
prior
\series default
 on 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Standard

\series bold
Bayesian/posterior risk
\series default
 conditions on the data: 
\begin_inset Formula 
\begin{alignat*}{1}
r(x,\pi)= & \int L(\theta,\delta(x))p(\theta\mid x)d\theta
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
Bayes risk
\series default
 can be obtained by integrating Bayesian risk:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
R(\delta,\pi)= & \int r(x,\pi)p(x)dx
\end{alignat*}

\end_inset


\end_layout

\begin_layout Subsection
Exponential Families
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\mu$
\end_inset

 be a measure on 
\begin_inset Formula $\mathbb{R}^{n},h:\mathbb{R}^{n}\rightarrow\mathbb{R}^{+},T_{1},...,T_{s}:\mathbb{R}^{n}\rightarrow\mathbb{R}$
\end_inset

 measurable and 
\begin_inset Formula $\eta\in\mathbb{R}^{s}.$
\end_inset

 The exponential family is defined as:
\begin_inset Formula 
\begin{alignat}{1}
p_{\eta}(x)= & \exp\left(\sum_{i=1}^{s}\eta_{i}T_{i}(x)-A(\eta)\right)h(x)\\
A(\eta)= & \log\int\exp\left(\sum_{i=1}^{s}\eta_{i}T_{i}(x)\right)h(x)\mu(dx)\label{eq:A(eta)}
\end{alignat}

\end_inset


\end_layout

\begin_layout Definition
The set 
\begin_inset Formula $\Xi=\left\{ \eta:A(\eta)<\infty\right\} $
\end_inset

 is called the natural parameter space, and the family of density 
\begin_inset Formula $p_{\eta}(x)$
\end_inset

 is called an s-parameter exponential family in canonical form.
 
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $\Xi_{f}=\left\{ \eta:\int\left|f(x)\right|\exp\left(\sum_{i}\eta_{i}T_{i}(x)\right)h(x)\mu(dx)<\infty\right\} .$
\end_inset

 Then:
\begin_inset Formula 
\begin{alignat*}{1}
g(\eta)= & \int f(x)\exp\left(\sum\eta_{i}T_{i}(x)\right)h(x)\mu(dx)
\end{alignat*}

\end_inset

 is continuous, had continuous partial derivatives of all order for 
\begin_inset Formula $\eta\in\Xi_{f}^{o},$
\end_inset

 the interior of 
\begin_inset Formula $\Xi_{f}.$
\end_inset

 Further more, these derivatives can be computed by differentiation under
 the integral sign.
 
\end_layout

\begin_layout Standard
E.g.
 when 
\begin_inset Formula $f=1,$
\end_inset

 by 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:A(eta)"

\end_inset

, 
\begin_inset Formula 
\begin{alignat*}{1}
g(\eta)=\exp\left(A(\eta)\right)= & \int\exp\left[\sum_{i=1}^{s}n_{i}T_{i}(x)\right]h(x)d\mu(x)\\
\Rightarrow\exp\left(A(\eta)\right)\frac{\partial A(\eta)}{\partial\eta_{j}}= & \int\frac{\partial}{\partial\eta_{j}}\exp\left[\sum_{i=1}^{s}\eta_{i}T_{i}(x)\right]h(x)d\mu(x)\\
= & \int T_{i}(x)\exp\left[\sum_{i=1}^{s}\eta_{i}T_{i}(x)\right]h(x)d\mu(x)\\
\Rightarrow\mathbb{E}_{\eta}T_{j}(x)= & \frac{\partial A(\eta)}{\partial\eta}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
The moment generating function, and cumulant generating function of a random
 vector T is defined as:
\begin_inset Formula 
\begin{alignat*}{1}
M_{T}(u)= & \mathbb{E}\exp\left(u^{T}T\right)\\
K_{T}(u)= & \log\mathbb{E}\exp\left(u^{T}T\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
Then 
\begin_inset Formula 
\begin{alignat*}{1}
\alpha_{r_{1},...,r_{s}}\coloneqq\mathbb{E}\left[T_{1}^{r_{1}}...T_{s}^{r_{s}}\right]= & \frac{\partial^{r_{1}}}{\partial u_{1}^{r_{1}}}...\frac{\partial^{r_{s}}}{\partial u_{s}^{r_{s}}}M_{T}(u)\mid_{u=0}\\
\kappa_{r_{1},...,r_{s}}\coloneqq & \frac{\partial^{r_{1}}}{\partial u_{1}^{r_{1}}}...\frac{\partial^{r_{s}}}{\partial u_{s}^{r_{s}}}K_{T}(u)\mid_{u=0}\\
K'_{T}= & \mathbb{E}T\\
K''_{T}= & \mathbb{E}T^{2}-\left(\mathbb{E}T\right)^{2}={\rm Var}\left(T\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
For the exponential family, 
\begin_inset Formula 
\begin{alignat*}{1}
M_{T}(u)= & \exp\left(A(u+\eta)-A(\eta)\right)\\
K_{T}(u)= & A(u+\eta)-A(\eta)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Subsection
Sufficient Statistics
\end_layout

\begin_layout Definition
(Frequentist sufficiency).
 Let 
\begin_inset Formula $X$
\end_inset

 be an rv with distribution from a family 
\begin_inset Formula $\mathcal{P}=\left\{ P_{\theta}:\theta\in\Omega\right\} .$
\end_inset

 Then 
\begin_inset Formula $T(X)$
\end_inset

 is a sufficient statistic for 
\begin_inset Formula $\mathcal{P}$
\end_inset

 iff, 
\begin_inset Formula $\forall t,\theta,$
\end_inset

 the conditional distribution 
\begin_inset Formula $P_{\theta}(X\mid T=t)$
\end_inset

 does not depend on 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition
(Bayesian sufficiency).
 A statistic 
\begin_inset Formula $T$
\end_inset

 is sufficient if 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 are conditionally independent given 
\begin_inset Formula $T(X)=t,$
\end_inset

 i.e., if
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[x,\theta\mid t\right]= & \mathbb{P}\left[x\mid t\right]\mathbb{P}\left[\theta\mid t\right]
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
For the exponential family, 
\begin_inset Formula $T(x)$
\end_inset

 is a sufficient statistic.
 
\end_layout

\begin_layout Theorem
Suppose that 
\begin_inset Formula $X\sim\mathcal{P}=\left\{ P_{\theta}:\theta\in\Omega\right\} $
\end_inset

 and that 
\begin_inset Formula $T$
\end_inset

 is sufficient for the family 
\begin_inset Formula $\mathcal{P}.$
\end_inset

 Then for any estimator 
\begin_inset Formula $\delta(X)$
\end_inset

 of 
\begin_inset Formula $g(\theta),$
\end_inset

 
\begin_inset Formula $\exists$
\end_inset

 a (possibly randomized) estimator based on 
\begin_inset Formula $T$
\end_inset

 that has the same risk function as 
\begin_inset Formula $\delta(X)$
\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Factorization.
 A statistic 
\begin_inset Formula $T$
\end_inset

 is sufficient if there exists 
\begin_inset Formula $g_{\theta}\ge0$
\end_inset

 and 
\begin_inset Formula $h\ge0$
\end_inset

 such that:
\begin_inset Formula 
\begin{alignat*}{1}
p_{\theta}(x)= & g_{\theta}(T(x))h(x)
\end{alignat*}

\end_inset

 almost everywhere.
 
\end_layout

\begin_layout Definition

\end_layout

\begin_layout Section
Large-Sample Theory
\end_layout

\begin_layout Definition
Convergence in Distribution.
 Let 
\begin_inset Formula $Y_{n}$
\end_inset

 be a sequence of random variables, 
\begin_inset Formula $H_{n}$
\end_inset

 be the CDF of 
\begin_inset Formula $Y_{n}$
\end_inset

 and H be the CDF of 
\begin_inset Formula $Y.$
\end_inset

 
\begin_inset Formula $Y_{n}\overset{d}{\rightarrow}Y$
\end_inset

 if 
\begin_inset Formula $H_{n}(y)\rightarrow H(y)$
\end_inset

 for every y at which 
\begin_inset Formula $H$
\end_inset

 is continuous.
 
\end_layout

\begin_layout Lemma
Implications
\end_layout

\begin_layout Lemma
1.
 
\begin_inset Formula $Y_{n}\overset{d}{\rightarrow}Y$
\end_inset

 iff 
\begin_inset Formula $\mathbb{E}f(Y_{n})\rightarrow\mathbb{E}f(Y)\forall$
\end_inset

 bounded, continuous f.
\end_layout

\begin_layout Lemma
2.
 
\begin_inset Formula $Y_{n}\overset{d}{\rightarrow}Y$
\end_inset

 and g is continuous 
\begin_inset Formula $\Rightarrow g(Y_{n})\overset{d}{\rightarrow}g(Y)$
\end_inset


\end_layout

\begin_layout Theorem
Central Limit Theorem.
 Given 
\begin_inset Formula $X_{i}$
\end_inset

 i.i.d with mean 
\begin_inset Formula $\mu,$
\end_inset

 variance 
\begin_inset Formula $\sigma^{2}.$
\end_inset

 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{n}(\bar{X}_{n}-\mu)\overset{d}{\rightarrow} & \mathcal{N}(0,\sigma^{2})
\end{alignat*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Slutsky's Theorem.
 Suppose 
\begin_inset Formula $Y_{n}\overset{d}{\rightarrow}Y,A_{n}\overset{P}{\rightarrow}a,B_{n}\overset{P}{\rightarrow}b.$
\end_inset

 Then:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
A_{n}+B_{n}Y_{n}\overset{d}{\rightarrow} & a+bY
\end{alignat*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Delta Method.
 Given 
\begin_inset Formula $X_{i}$
\end_inset

 i.i.d with mean 
\begin_inset Formula $\mu,$
\end_inset

 variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 f is differentiable at 
\begin_inset Formula $\mu.$
\end_inset

 Then
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{n}(f(\bar{X}_{n})-f(\mu))\overset{d}{\rightarrow} & \mathcal{N}(0,(f'(\mu))^{2}\sigma^{2})
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
Uniform Integrability
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{alignat*}{1}
\sup_{n\ge1}\mathbb{E}\left[\left|X_{n}\right|\mathbb{I}_{\left|X_{n}\right|\ge t}\right]\rightarrow0 & \mbox{as t \rightarrow\infty}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
\begin_inset Formula $X_{n}\overset{d}{\rightarrow}X$
\end_inset

 and uniform integrability 
\begin_inset Formula $\Rightarrow\mathbb{E}\left[X_{n}\right]\rightarrow\mathbb{E}\left[X\right]$
\end_inset

 
\end_layout

\begin_layout Standard
Note that this theorem is not a result of Lemma 1, because the function
 
\begin_inset Formula $f(x)=x$
\end_inset

 is not bounded.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $\hat{\eta}_{MLE}$
\end_inset

 be the MLE estimate for the 
\begin_inset Formula $\eta$
\end_inset

 parameter of exponential family.
 Then:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{n}\left(\hat{\eta}_{MLE}-\eta\right)\overset{d}{\rightarrow} & \mathcal{N}\left(0,\frac{1}{A''(\eta)}\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
MLE achieves Cramer-Rao in an asymptotic sense.
 Except not quite because of super efficicency.
 
\end_layout

\begin_layout Theorem
Keener 8.18.
 Let 
\begin_inset Formula $\left(X_{i}\right)_{i\ge1}$
\end_inset

 be i.i.d rv with common CDF 
\begin_inset Formula $F,$
\end_inset

 let 
\begin_inset Formula $\gamma\in(0,1)$
\end_inset

, and let 
\begin_inset Formula $\tilde{\theta}_{n}$
\end_inset

 be the 
\begin_inset Formula $\lfloor\gamma n\rfloor'th$
\end_inset

 order statistics for 
\begin_inset Formula $X_{1},X_{2},...,X_{n}$
\end_inset

.
 If 
\begin_inset Formula $F(\theta)=\gamma,$
\end_inset

 and if 
\begin_inset Formula $F'(\gamma)$
\end_inset

 exists and is finite and positive, then:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{n}(\tilde{\theta}-\theta)\overset{d}{\rightarrow} & \mathcal{N}\left(0,\frac{\gamma(1-\gamma)}{\left[F'(\theta)\right]^{2}}\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Section
Estimating Equations and Maximum Likelihood
\end_layout

\begin_layout Subsection
Weak Law for Random Functions
\end_layout

\begin_layout Definition
Random Element.
 Let 
\begin_inset Formula $(\Omega,\mathcal{F},\mathbb{P})$
\end_inset

 be a probability space, and 
\begin_inset Formula $(E,\mathcal{E})$
\end_inset

 a measurable space.
 A random element with value in 
\begin_inset Formula $E$
\end_inset

 is a function 
\begin_inset Formula $X:\Omega\rightarrow E$
\end_inset

 which is 
\begin_inset Formula $(\mathcal{F},\mathcal{E})-measurable.$
\end_inset

 
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Standard
Random Function is an example of random element.
 Let 
\begin_inset Formula $K$
\end_inset

 be a compact set.
 Let 
\begin_inset Formula $W_{i}(t)=h(t,X_{i}),t\in K$
\end_inset

.
 Assume 
\begin_inset Formula $h(t,x)$
\end_inset

 is continuous in t, 
\begin_inset Formula $\forall x.$
\end_inset

 
\begin_inset Formula $W_{i}$
\end_inset

 are random functions taking values in 
\begin_inset Formula $C(K),$
\end_inset

 the set of continuous function.
 
\end_layout

\begin_layout Definition
Supremum Norm.
 For 
\begin_inset Formula $w\in C(K)$
\end_inset

, the supremum norm of 
\begin_inset Formula $\omega$
\end_inset

 is defined as:
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{alignat*}{1}
\|w\|_{\infty}= & \sup_{t\in K}\left|w(t)\right|
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
Convergence means 
\begin_inset Formula $\|w_{n}-w\|_{\infty}\rightarrow0$
\end_inset

.
 
\end_layout

\begin_layout Lemma
Keener p152.
 Let 
\begin_inset Formula $W$
\end_inset

 be a random function in 
\begin_inset Formula $C(k).$
\end_inset

Define 
\begin_inset Formula $\mu(t)=\mathbb{E}W(t).t\in K$
\end_inset

.
 If 
\begin_inset Formula $\mathbb{E}\|W\|_{\infty}<\infty,$
\end_inset

 then 
\begin_inset Formula $\mu$
\end_inset

 is continuous.
 Also:
\end_layout

\begin_layout Lemma
\begin_inset Formula 
\begin{alignat*}{1}
\sup_{t\in K}\mathbb{E}\sup_{s:\|s-t\|<\epsilon}\left|W(s)-W(t)\right|\rightarrow & 0
\end{alignat*}

\end_inset

 as 
\begin_inset Formula $\epsilon\rightarrow0.$
\end_inset

 
\end_layout

\begin_layout Theorem
Dini's Theorem.
 Suppose 
\begin_inset Formula $f_{1}\ge f_{2}\ge...$
\end_inset

 are positive functions in 
\begin_inset Formula $C(K).$
\end_inset

 If 
\begin_inset Formula $f_{n}(x)\rightarrow0,\forall x\in K,$
\end_inset

 then 
\begin_inset Formula $\sup_{x\in K}f_{n}(K)\rightarrow0.$
\end_inset

 
\end_layout

\begin_layout Standard
Dini theorem turns pointwise statement into uniform statement.
 A more general form in Empirical Process Theory is:
\end_layout

\begin_layout Theorem
Uniform Weak Law.
 Keener p153.
 Let 
\begin_inset Formula $W,W_{1},W_{2},...$
\end_inset

 be i.i.d random functions in 
\begin_inset Formula $C(K),$
\end_inset

 K compact, with mean 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\mathbb{E}\|W\|_{\infty}<\infty$
\end_inset

.
 Let 
\begin_inset Formula $\bar{W}_{n}=\frac{1}{n}\sum_{i=1}^{n}W_{i}$
\end_inset

.
 This implies 
\begin_inset Formula $\|\bar{W}_{n}-\mu\|_{\infty}\overset{\mathbb{P}}{\rightarrow}0.$
\end_inset

 
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $G_{n},n\ge1,$
\end_inset

 be random function in 
\begin_inset Formula $C(K),$
\end_inset

 K compact, and suppose 
\begin_inset Formula $\|G_{n}-g\|_{\infty}\overset{\mathbb{P}}{\rightarrow}0$
\end_inset

 with g a nonrandom function in 
\begin_inset Formula $C(K).$
\end_inset

 
\end_layout

\begin_layout Theorem
1.
 If 
\begin_inset Formula $t_{n},n\ge1$
\end_inset

 are random variables converging in probability to a constant 
\begin_inset Formula $t^{*}\in K,t_{n}\overset{\mathbb{P}}{\rightarrow}t^{*},$
\end_inset

 then 
\begin_inset Formula $G_{n}(t_{n})\overset{\mathbb{P}}{\rightarrow}g(t^{*}).$
\end_inset

 
\end_layout

\begin_layout Theorem
2.
 If g achieves its maximum at a unique value 
\begin_inset Formula $t^{*},$
\end_inset

 and if 
\begin_inset Formula $t_{n}$
\end_inset

 are random variables maximizing 
\begin_inset Formula $G_{n},$
\end_inset

 so that: 
\begin_inset Formula $G_{n}(t_{n})=\sup_{t\in K}G_{n}(t),$
\end_inset

 then 
\begin_inset Formula $t_{n}\overset{\mathbb{P}}{\rightarrow}t^{*}.$
\end_inset

 
\end_layout

\begin_layout Theorem
3.
 If 
\begin_inset Formula $K\subset\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $g(t)=0$
\end_inset

 has a uninque solution 
\begin_inset Formula $t^{*},$
\end_inset

 and if 
\begin_inset Formula $t_{n}$
\end_inset

 are random variables solving 
\begin_inset Formula $G_{n}(t_{n})=0,$
\end_inset

 then 
\begin_inset Formula $t_{n}\overset{\mathbb{P}}{\rightarrow}t^{*}.$
\end_inset

 
\end_layout

\begin_layout Subsection
Consistency of The MLE
\end_layout

\begin_layout Standard
For this subsection, let 
\begin_inset Formula $X,X_{1},X_{2},...$
\end_inset

 be i.i.d with common density 
\begin_inset Formula $f_{\theta},\theta\in\Omega,$
\end_inset

 and let 
\begin_inset Formula $l_{n}$
\end_inset

 be the log-likelihood function for the first n obs:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
l_{n}(\omega)= & \sum_{i=1}^{n}\log f_{\omega}(X_{i})
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
The MLE estimator 
\begin_inset Formula $\hat{\theta}_{n}=\hat{\theta}_{n}(X_{1},...,X_{n})$
\end_inset

 maximize 
\begin_inset Formula $l_{n}.$
\end_inset

 Assuming 
\begin_inset Formula $f_{\theta}(x)$
\end_inset

 is continuous in 
\begin_inset Formula $\theta.$
\end_inset

 
\end_layout

\begin_layout Definition
The Kullback-Leiber information is defined as:
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{alignat*}{1}
I(\theta,\omega)= & \mathbb{E}\log\left[f_{\theta}(X)/f_{\omega}(X)\right]
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
Here 
\begin_inset Formula $\theta$
\end_inset

 is thought of as the true value of the unknown parameter, 
\begin_inset Formula $\omega$
\end_inset

 is a dummy variable.
 This information is viewed as a measure of the information discriminating
 between 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

 when 
\begin_inset Formula $\theta$
\end_inset

 is the true value of the unknown parameter.
 
\end_layout

\begin_layout Lemma
If 
\begin_inset Formula $P_{\theta}\neq P_{\omega},$
\end_inset

 then 
\begin_inset Formula $I(\theta,\omega)>0.$
\end_inset

 
\end_layout

\begin_layout Theorem
Define 
\begin_inset Formula $W(\omega)=\log\left[\frac{f_{\omega}(X)}{f_{\theta}(X)}\right].$
\end_inset

 If 
\begin_inset Formula $\Omega$
\end_inset

 is compact, 
\begin_inset Formula $\mathbb{E}_{\theta}\|W\|_{\infty}<\infty,f_{\omega}(x)$
\end_inset

 is continuous function of 
\begin_inset Formula $\omega$
\end_inset

 for a.e.
 x, and 
\begin_inset Formula $P_{\omega}\neq P_{\omega},\forall\omega\neq\theta,$
\end_inset

 then under 
\begin_inset Formula $P_{\theta},\hat{\theta}_{n}\overset{\mathbb{P}}{\rightarrow}\theta.$
\end_inset

 
\end_layout

\begin_layout Standard
Note that here 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is the MLE for n observation, which mean it maximize 
\begin_inset Formula $W(\omega).$
\end_inset

 This theorem establishes the consistency of MLE.
 
\end_layout

\begin_layout Theorem
Suppose 
\begin_inset Formula $\Omega=\mathbb{R}^{p},$
\end_inset

 
\begin_inset Formula $f_{\omega}(x)$
\end_inset

 is a continuous function of 
\begin_inset Formula $\omega$
\end_inset

 for a.e.
 x, 
\begin_inset Formula $P_{\omega}\neq P_{\theta}$
\end_inset

 for all 
\begin_inset Formula $\omega\neq\theta,$
\end_inset

 and 
\begin_inset Formula $f_{\omega}(x)\rightarrow0$
\end_inset

 as 
\begin_inset Formula $\omega\rightarrow\infty.$
\end_inset

 If 
\begin_inset Formula $\mathbb{E}_{\theta}\|\mathbb{I}_{K}W\|_{\infty}<\infty$
\end_inset

 for any compact set 
\begin_inset Formula $K\subset\mathbb{R}^{p},$
\end_inset

 and if 
\begin_inset Formula $\mathbb{E}_{\theta}\sup_{\|\omega\|>a}W(\omega)<\infty$
\end_inset

 for some 
\begin_inset Formula $a>0,$
\end_inset

 then under 
\begin_inset Formula $\mathbb{P}_{\theta},\hat{\theta}_{n}\overset{\mathbb{P}}{\rightarrow}\theta.$
\end_inset

 
\end_layout

\begin_layout Subsection
Limiting Distribution for the MLE
\end_layout

\begin_layout Theorem
Keener 9.14.
 Assume:
\end_layout

\begin_layout Theorem
1.
 RV 
\begin_inset Formula $X,X_{1},X_{2},...$
\end_inset

 are i.i.d with common density 
\begin_inset Formula $f_{\theta},\theta\in\Omega\subset\mathbb{R}$
\end_inset

 
\end_layout

\begin_layout Theorem
2.
 The set 
\begin_inset Formula $A=\left\{ x\mid f_{\theta}(x)>0\right\} $
\end_inset

 is independent of 
\begin_inset Formula $\theta$
\end_inset


\end_layout

\begin_layout Theorem
3.
 
\begin_inset Formula $\forall x\in A,\partial^{2}f_{\theta}(x)/\partial\theta^{2}$
\end_inset

 exists and is continuous in 
\begin_inset Formula $\theta$
\end_inset


\end_layout

\begin_layout Theorem
4.
 Let 
\begin_inset Formula $W(\theta)=\log f_{\theta}(X).$
\end_inset

 The Fisher information 
\begin_inset Formula $I(\theta)$
\end_inset

 from a single observation exists, is finite and can be found using either:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
I(\theta)= & \mathbb{E}_{\theta}W'(\theta)^{y2},\mbox{ or}\\
I(\theta)= & -\mathbb{E}_{\theta}W''(\theta)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
5.
 
\begin_inset Formula $\forall\theta$
\end_inset

 in the interior of 
\begin_inset Formula $\Omega,$
\end_inset

 
\begin_inset Formula $\exists\epsilon>0$
\end_inset

 such that:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}_{\theta}\|\mathbb{I}_{\left[\theta-\epsilon,\theta+\epsilon\right]}W''\|_{\infty}< & \infty
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
6.
 The maximum likelihood estimator 
\begin_inset Formula $\hat{\theta}_{n}$
\end_inset

 is consistent
\end_layout

\begin_layout Theorem
Then for any 
\begin_inset Formula $\theta$
\end_inset

 in the interior of 
\begin_inset Formula $\Omega,$
\end_inset

 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{n}\left(\hat{\theta}_{n}-\theta\right)\overset{d}{\rightarrow} & \mathcal{N}\left(0,\frac{1}{I(\theta)}\right)
\end{alignat*}

\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 
\end_layout

\begin_layout Lemma
Suppose 
\begin_inset Formula $Y_{n}\overset{d}{\rightarrow}Y,$
\end_inset

 and 
\begin_inset Formula $\mathbb{P}(B_{n})\rightarrow1$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 Then for arbitrary rv 
\begin_inset Formula $Z_{n},n\ge1,Y_{n}\mathbb{I}_{B_{n}}+Z_{n}\mathbb{I}_{B_{n}^{C}}\overset{d}{\rightarrow}Y$
\end_inset

 as 
\begin_inset Formula $n\rightarrow\infty.$
\end_inset

 
\end_layout

\begin_layout Subsection
Confidence Intervals
\end_layout

\begin_layout Definition
If 
\begin_inset Formula $\delta_{0}$
\end_inset

 and 
\begin_inset Formula $\delta_{1}$
\end_inset

 are statistics, then the random interval 
\begin_inset Formula $(\delta_{0},\delta_{1})$
\end_inset

 is called a 
\begin_inset Formula $1-\alpha$
\end_inset

 confidence interval for 
\begin_inset Formula $g(\theta)$
\end_inset

 iff:
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}_{\theta}(g(\theta)\in(\delta_{0},\delta_{1}))\ge & 1-\alpha
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
for all 
\begin_inset Formula $\theta\in\Omega.$
\end_inset

 Also, a random set 
\begin_inset Formula $S=S(X)$
\end_inset

 constructed from data 
\begin_inset Formula $X$
\end_inset

 is called a 
\begin_inset Formula $1-\alpha$
\end_inset

 confidence region for 
\begin_inset Formula $g(\theta)$
\end_inset

 if:
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}_{\theta}(g(\theta)\in S)\ge & 1-\alpha
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
for all 
\begin_inset Formula $\theta\in\Omega.$
\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Definition
A variable, which depends on both the data and the parameter, but whose
 distribution is independent of the parameter is called Pivot
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $\sqrt{n}(\hat{\theta}_{n}-\theta)\Rightarrow\mathcal{N}(0,1/I(\theta))$
\end_inset

.
 Then
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{nI(\theta)}(\hat{\theta}_{n}-\theta)\Rightarrow & \mathcal{N}(0,1)\\
\Rightarrow\mathbb{P}\left[\sqrt{nI(\theta)}\left|\hat{\theta}_{n}-\theta\right|\le z_{\alpha/2}\right]\rightarrow & 1-\alpha
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
It is often difficult to calculate Fisher information.
 We will discuss strategies to approximate the Fisher information
\end_layout

\begin_layout Enumerate
We can use 
\begin_inset Formula $I(\hat{\theta}_{n})$
\end_inset

 instead.
 If 
\begin_inset Formula $I(\theta)$
\end_inset

 is continuous then:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{\frac{I(\hat{\theta}_{n})}{I(\theta)}}\rightarrow^{p} & 1
\end{alignat*}

\end_inset

Then using Slutsky theory, we can conclude that
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{nI(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)= & \sqrt{\frac{I(\hat{\theta}_{n})}{I(\theta)}}\sqrt{nI(\theta)}(\hat{\theta}_{n}-\theta)\Rightarrow\mathcal{N}(0,1)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Enumerate
We can also use the results from empirical process theorey.
 Remember that 
\begin_inset Formula $l(\theta)=\sum_{i=1}^{n}\log f_{\theta}(X_{i}).$
\end_inset

 Thus by law of large number
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
\frac{-l''(\hat{\theta}_{n})}{n}\rightarrow^{p} & I(\theta)
\end{alignat*}

\end_inset

And thus by Slutsky theorem
\begin_inset Formula 
\begin{alignat*}{1}
\sqrt{-l''(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)\Rightarrow & \mathcal{N}(0,1)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Enumerate
Another method are profile regions.
 Expand 
\begin_inset Formula $l_{n}(\theta)$
\end_inset

 in Taylor series
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
l_{n}(\theta)= & l_{n}(\hat{\theta}_{n})+\frac{1}{2}l''_{n}(\theta_{n}^{*})(\theta-\hat{\theta}_{n})^{2}
\end{alignat*}

\end_inset

Here 
\begin_inset Formula $\theta_{n}^{*}$
\end_inset

 is a random variable between 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\hat{\theta}_{n}.$
\end_inset

 By rearranging this equation we get
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
2l_{n}(\hat{\theta}_{n})-2l_{n}(\theta)= & \left[\sqrt{-l''_{n}(\theta_{n}^{*})}(\theta-\hat{\theta}_{n})\right]^{2}\Rightarrow\mathcal{X}_{1}^{2}
\end{alignat*}

\end_inset

Note that for 
\begin_inset Formula $Z\sim\mathcal{N}(0,1)$
\end_inset

 
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[Z^{2}\le z_{\alpha/2}^{2}\right]= & \mathbb{P}\left[-z_{\alpha/2}\le Z\le z_{\alpha/2}\right]=1-\alpha
\end{alignat*}

\end_inset

And thus
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}_{\theta}(2l_{n}(\hat{\theta}_{n})-2l_{n}(\theta)\le z_{\alpha/2}^{2})\rightarrow & 1-\alpha
\end{alignat*}

\end_inset

This identity can now be used to calculate the asymptotic 
\begin_inset Formula $(1-\alpha)$
\end_inset


\begin_inset Newline newline
\end_inset

confidence interval for 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Section
Hypothesis Testing
\end_layout

\begin_layout Subsection
General Setting and Simple vs.
 Simple Case
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\mathcal{P}=\left\{ P_{\theta},\theta\in\Omega\right\} $
\end_inset

 be a family of distribution.
 The distributions of 
\begin_inset Formula $\mathcal{P}$
\end_inset

 can be classified into 
\begin_inset Formula $H$
\end_inset

, one in which the hypothesis is true, and 
\begin_inset Formula $K$
\end_inset

, one in which the hypothesis is false.
 And the corresponding partition of 
\begin_inset Formula $\Omega$
\end_inset

 into 
\begin_inset Formula $\Omega_{H},\Omega_{K}$
\end_inset

.
 
\begin_inset Formula $H\cup K=\mathcal{P},\Omega_{H}\cup\Omega_{k}=\mathcal{P}.$
\end_inset


\end_layout

\begin_layout Standard
Let the decision of accepting or rejecting 
\begin_inset Formula $H$
\end_inset

 be 
\begin_inset Formula $d_{0}$
\end_inset

 and 
\begin_inset Formula $d_{1}$
\end_inset

 respectively.
 A nonrandomized test procedure assigns to each possible value 
\begin_inset Formula $x$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 one of these two decisions and thereby divides the sample space into two
 complementary regions 
\begin_inset Formula $S_{0}$
\end_inset

 and 
\begin_inset Formula $S_{1}.$
\end_inset

 The set 
\begin_inset Formula $S_{0}$
\end_inset

 is called the region of acceptance, and the set 
\begin_inset Formula $S_{1}$
\end_inset

 the region of rejection or critical region.
 
\end_layout

\begin_layout Standard
There are two types of error: rejecting the hypothesis when it is true (Type
 I), and accepting when it is false (Type II).
 
\end_layout

\begin_layout Standard
Define the level of significant 
\begin_inset Formula $\alpha$
\end_inset

 be such that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat}{1}
P_{\theta}(\delta(X)=d_{1})=P_{\theta}(X\in S_{1})\le\alpha, & \forall\theta\in\Omega_{H}\label{eq:1}
\end{alignat}

\end_inset


\end_layout

\begin_layout Standard
Subject to this condition, it is desired to minimize 
\begin_inset Formula $P_{\theta}(\delta(X)=d_{0})$
\end_inset

 for 
\begin_inset Formula $\theta\in\Omega_{K}$
\end_inset

 or equivalently maximize
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat}{1}
P_{\theta}(\delta(X)=d_{1})=P_{\theta}(X\in S_{1}) & ,\forall\theta\in\Omega_{K}
\end{alignat}

\end_inset


\end_layout

\begin_layout Standard
Usually, (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1"

\end_inset

) implies
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat}{1}
\sup_{\Omega_{H}}P_{\theta}(X\in S)= & \alpha
\end{alignat}

\end_inset


\end_layout

\begin_layout Standard
The LHS in (3) is called the size of the test or critical region 
\begin_inset Formula $S_{1}.$
\end_inset

 The probability of rejection in (2) is called power of the test against
 the alternative 
\begin_inset Formula $\theta$
\end_inset

.
 Consider as a function of 
\begin_inset Formula $\theta$
\end_inset

 over 
\begin_inset Formula $\Omega$
\end_inset

, the probability is called the power function of the test and is denoted
 
\begin_inset Formula $\beta(\theta).$
\end_inset


\end_layout

\begin_layout Standard
Now, for each 
\begin_inset Formula $X=x,$
\end_inset

 instead of choosing 
\begin_inset Formula $1$
\end_inset

 or 
\begin_inset Formula $0$
\end_inset

 deterministically, one can do it randomly as a Bernoulli trial with success
 rate 
\begin_inset Formula $\phi(x).$
\end_inset

 The randomized test is therefore completely characterized with the function
 
\begin_inset Formula $\phi(x).$
\end_inset

The set of points 
\begin_inset Formula $x$
\end_inset

 for which 
\begin_inset Formula $\phi(x)=1$
\end_inset

 is the region of rejection.
 The probability of rejection is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}_{\theta}\phi(X)= & \int\phi(x)dP_{\theta}(x)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
The problem now is to select 
\begin_inset Formula $\phi$
\end_inset

 so as to maximize the power
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat}{1}
\beta_{\phi}(\theta)= & \mathbb{E}_{\theta}\phi(X),\forall\theta\in\Omega_{K}
\end{alignat}

\end_inset


\end_layout

\begin_layout Standard
subject to the condition
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat}{1}
\mathbb{E}_{\theta}\phi(X)\le & \alpha,\forall\theta\in\Omega_{H}
\end{alignat}

\end_inset


\end_layout

\begin_layout Standard
Now if 
\begin_inset Formula $\Omega_{K}$
\end_inset

 has more than one element, the test that maximize the power for each alternativ
e will be different, so things is more complicated.
 But if 
\begin_inset Formula $\Omega_{K}$
\end_inset

 has only one element, things simplify.
 Sometimes in case when there are many alternatives, we can get lucky and
 have the test maximizes the power of all alternatives in K.
 This is called 
\series bold
uniformly most powerful
\series default
 UMP tests.
 
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $P_{0}$
\end_inset

 and 
\begin_inset Formula $P_{1}$
\end_inset

 be probability distribution possessing densities 
\begin_inset Formula $p_{0}$
\end_inset

 and 
\begin_inset Formula $p_{1}$
\end_inset

 respectively wrt measure 
\begin_inset Formula $\mu.$
\end_inset


\end_layout

\begin_layout Theorem
(i) Existence.
 For testing H: 
\begin_inset Formula $p_{0}$
\end_inset

 against the alternative K: 
\begin_inset Formula $p_{1}$
\end_inset

 there exists a test 
\begin_inset Formula $\phi$
\end_inset

 and a constant k such that: 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat}{1}
\mathbb{E}_{0}\phi(X)= & \alpha\label{eq:6}
\end{alignat}

\end_inset


\end_layout

\begin_layout Theorem
and 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat}{1}
\phi(x)= & \begin{cases}
1 & \mbox{when }p_{1}(x)>kp_{0}(x)\\
0 & \mbox{when }p_{1}(x)<kp_{0}(x)
\end{cases}\label{eq:7}
\end{alignat}

\end_inset


\end_layout

\begin_layout Theorem
(ii) Sufficient Condition for a Most Powerful Test.
 If a test sastifies (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:6"

\end_inset

), and (7) for some k, then it is most powerful for testing 
\begin_inset Formula $p_{0}$
\end_inset

 against 
\begin_inset Formula $p_{1}$
\end_inset

 at level 
\begin_inset Formula $\alpha.$
\end_inset


\end_layout

\begin_layout Theorem
(iii) Necessary condition for most powerful test.
 If 
\begin_inset Formula $\phi$
\end_inset

 is most powerful at level 
\begin_inset Formula $\alpha$
\end_inset

 for testing 
\begin_inset Formula $p_{0}$
\end_inset

 against 
\begin_inset Formula $p_{1},$
\end_inset

 then for some k it sastitifes 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:7"

\end_inset

 a.e.
 
\begin_inset Formula $\mu.$
\end_inset

 It also sastifies 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:6"

\end_inset

 unless there exists a test of size 
\begin_inset Formula $<\alpha$
\end_inset

 and with power 1.
 
\end_layout

\begin_layout Subsection
Simple vs Multiple - Distributions with Monotone Likelihood Ratio
\end_layout

\begin_layout Standard
Now when the set 
\begin_inset Formula $\Omega_{K}$
\end_inset

 has multiple elements, again in general the most powerful test of H agains
 an alternative 
\begin_inset Formula $\theta_{1}>\theta_{0}$
\end_inset

 (in contrast to 
\begin_inset Formula $H_{0}$
\end_inset

 
\begin_inset Formula $\theta_{1}\le\theta_{0}$
\end_inset

) depends on 
\begin_inset Formula $\theta_{1}$
\end_inset

 and is then not UMP.
 However, a UMP test does exists if an additional assumption is satisfied.
 The real-parameter family of densities 
\begin_inset Formula $p_{\theta}(x)$
\end_inset

 is said to have monotone likelihood ratio 
\end_layout

\begin_layout Definition
Monotone Likelihood Ratio.
 The real-parameter family of densities 
\begin_inset Formula $p_{\theta}(x)$
\end_inset

 is said to have monotone likelihood ratio if there exists a real-valued
 function 
\begin_inset Formula $T(x)$
\end_inset

 such that for any 
\begin_inset Formula $\theta<\theta'$
\end_inset

 the distribution 
\begin_inset Formula $P_{\theta}$
\end_inset

 and 
\begin_inset Formula $P_{\theta'}$
\end_inset

 are distinct, and the ratio 
\begin_inset Formula $p_{\theta'}(x)/p_{\theta}(x)$
\end_inset

 is a nondecreasing function of 
\begin_inset Formula $T(x).$
\end_inset


\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $\theta$
\end_inset

 be a real parameter, and let the random variable 
\begin_inset Formula $X$
\end_inset

 have density 
\begin_inset Formula $p_{\theta}(x)$
\end_inset

 with monotone likelihood ratio in 
\begin_inset Formula $T(x).$
\end_inset


\end_layout

\begin_layout Theorem
(i) For testing 
\begin_inset Formula $H:\theta\le\theta_{0}$
\end_inset

 against 
\begin_inset Formula $K:\theta>\theta_{0},$
\end_inset

 there exists a UMP test, which is given by:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat}{1}
\phi(x)= & \begin{cases}
1 & ,T(x)>C\\
\gamma & ,T(x)=C\\
0 & ,T(x)<C
\end{cases}\label{eq:8}
\end{alignat}

\end_inset


\end_layout

\begin_layout Theorem
where 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 are determined by
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat}{1}
\mathbb{E}_{\theta_{0}}\phi(X)= & \alpha\label{eq:9}
\end{alignat}

\end_inset


\end_layout

\begin_layout Theorem
(ii) The power function 
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\beta(\theta)= & \mathbb{E}_{\theta}\phi(X)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
of this test is strictly increasing for all points 
\begin_inset Formula $\theta$
\end_inset

 for which 
\begin_inset Formula $0<\beta(\theta)<1$
\end_inset


\end_layout

\begin_layout Theorem
(iii) For all 
\begin_inset Formula $\theta',$
\end_inset

 the test determined by (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:8"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:9"

\end_inset

) is UMP for testing 
\begin_inset Formula $H':\theta\le\theta'$
\end_inset

 against 
\begin_inset Formula $K':\theta>\theta'$
\end_inset

 at level 
\begin_inset Formula $\alpha'=\beta(\theta')$
\end_inset

 
\end_layout

\begin_layout Theorem
(iv) For any 
\begin_inset Formula $\theta<\theta_{0}$
\end_inset

 the test minimizes 
\begin_inset Formula $\beta(\theta)$
\end_inset

 (Type I error) amonng all tests satisfying (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:9"

\end_inset

).
 
\end_layout

\begin_layout Section
Concentration Bound
\end_layout

\begin_layout Theorem
Markov Inequality.
 
\begin_inset Formula $X\ge0.\mathbb{E}X<\infty.$
\end_inset

 Then:
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[X\ge t\right]\le & \frac{\mathbb{E}X}{t},t>0
\end{alignat*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Chebyshev.
 
\begin_inset Formula $\mathbb{E}X<\infty$
\end_inset

.
 Then: 
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[\left|X-\mu\right|\ge t\right]\le & \frac{\mathbb{E}\left(X-\mu\right)^{2}}{t^{2}}
\end{alignat*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Assuming 
\begin_inset Formula $\mathbb{E}\exp\left\{ \lambda\left(X-\mu\right)\right\} <\infty$
\end_inset

 for 
\begin_inset Formula $\left|\lambda\right|\le b$
\end_inset

.
 Then
\begin_inset Formula $\forall\lambda\in\left[0,b\right]:$
\end_inset


\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[X-\mu\ge t\right]\le & \frac{\mathbb{E}\exp\left\{ \lambda\left(X-\mu\right)\right\} }{\exp\left[\lambda t\right]}\\
\mathbb{P}\left[X-\mu\ge t\right]\le & -\sup_{\lambda\in\left[0,b\right]}\left\{ \lambda t-\log\mathbb{E}\exp\left\{ \lambda\left(X-\mu\right)\right\} \right\} 
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
E.g.
 Gaussian: Mgf: 
\begin_inset Formula $\mathbb{E}\exp\left\{ \lambda X\right\} =\exp\left\{ \mu\lambda+\lambda^{2}\sigma^{2}/2\right\} ,\forall\lambda\in\mathbb{R}$
\end_inset

.
 Thus: 
\begin_inset Formula $\mathbb{P}\left[X-\mu\ge t\right]\le\exp\left\{ -t^{2}/\left(2\sigma^{2}\right)\right\} $
\end_inset


\end_layout

\begin_layout Definition
Sub-gaussian.
 A RV 
\begin_inset Formula $X$
\end_inset

 with mean 
\begin_inset Formula $\mu$
\end_inset

 is called sub-gaussian iff 
\begin_inset Formula $\exists\sigma>0$
\end_inset

 such that:
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}\exp\left\{ \lambda\left(X-\mu\right)\right\} \le\exp\left\{ \frac{\lambda^{2}\sigma^{2}}{2}\right\} ,\forall\lambda\in\mathbb{R}
\end{alignat*}

\end_inset

 
\begin_inset Formula $\sigma$
\end_inset

 is the sub-gaussian parameter.
 
\end_layout

\begin_layout Standard
The symmetry of definition implies 
\begin_inset Formula $X$
\end_inset

 is sub-gaussian iff 
\begin_inset Formula $-X$
\end_inset

 is sub-gaussian.
 Thus
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[\left|X-\mu\right|\ge t\right]\le & 2\exp\left\{ -\frac{t^{2}}{2\sigma^{2}}\right\} 
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
Hoeffding Bound.
 
\begin_inset Formula $X_{i}$
\end_inset

 mean 
\begin_inset Formula $\mu_{i}$
\end_inset

 and subgaussian with parameter 
\begin_inset Formula $\sigma_{i}.$
\end_inset

 Then 
\begin_inset Formula $\forall t\ge0,$
\end_inset

 
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[\sum_{i=1}^{n}\left(X_{i}-\mu_{i}\right)\ge t\right]\le & \exp\left\{ -\frac{t^{2}}{-2\sum_{i=1}^{n}\sigma_{i}^{2}}\right\} 
\end{alignat*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Theorem
Equivalent characterizations of sub-Gaussian variables.
 Given any zero-mean RV X, the following properties are equivalent:
\begin_inset Formula 
\begin{alignat*}{1}
(i) & \exists\sigma:\mathbb{E}\left[\exp\left\{ \lambda X\right\} \right]\le\exp\left\{ \frac{\lambda^{2}\sigma^{2}}{2}\right\} \\
(ii) & \exists c\ge1,Z\sim\mathcal{N}\left(0,\tau^{2}\right):\mathbb{P}\left[\left|X\right|\ge s\right]\le c\mathbb{P}\left[\left|Z\right|\ge s\right],\forall s\ge0\\
(iii) & \exists\theta\ge0:\mathbb{E}\left[X^{2k}\right]\le\frac{(2k)!}{2^{k}k!}\theta^{2k},\forall k=1,2,...\\
(iv) & \mathbb{E}\left[\exp\left\{ \frac{\lambda X^{2}}{2\sigma^{2}}\right\} \le\frac{1}{\sqrt{1-\lambda}}\right],\forall\lambda\in[0,1)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
A RV X with mean 
\begin_inset Formula $\mu$
\end_inset

 is sub-exponential if 
\begin_inset Formula $\exists\nu>0,b>0$
\end_inset

 such that:
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}\exp\left\{ \lambda\left(X-\mu\right)\right\} \le & \exp\left\{ \frac{\nu^{2}\lambda^{2}}{2}\right\} ,\forall\left|\lambda\right|\le\frac{1}{b}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
Sub-exponential tail bound.
 Suppose that 
\begin_inset Formula $X$
\end_inset

 is sub-exponential with parameters 
\begin_inset Formula $\left(\nu,b\right).$
\end_inset

 Then:
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[X\ge\mu+t\right]\le & \begin{cases}
\exp\left\{ -\frac{t^{2}}{2\nu^{2}}\right\} , & \forall t\in\left[0,\frac{\nu^{2}}{b}\right]\\
\exp\left\{ -\frac{t}{2b}\right\} , & \forall t>\frac{\nu^{2}}{b}
\end{cases}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Definition
Bernstein's condition.
 Given a RV 
\begin_inset Formula $X$
\end_inset

 mean 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}.$
\end_inset

 We say that Bernstein's condition with parameter b holds if:
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}\left[\left(X-\mu\right)^{k}\right]\le & \frac{1}{2}k!\sigma^{2}b^{k-2},\forall k=3,4,...
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
Bernstein-type bound.
 For any RV satisfying the Bernstein condition above, we have:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}\left[\exp\left\{ \lambda\left(X-\mu\right)\right\} \right]\le & \exp\left\{ \frac{\lambda^{2}\sigma^{2}/2}{1-b\left|\lambda\right|}\right\} ,\forall\left|\lambda\right|<\frac{1}{b}\\
\mathbb{P}\left[\left|X-\mu\right|\ge t\right]\le & 2\exp\left\{ -\frac{t^{2}}{2\left(\sigma^{2}+bt\right)}\right\} 
\end{alignat*}

\end_inset


\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Standard
Multivariate sub-exponential.
 RV's 
\begin_inset Formula $X_{1},...,X_{n}$
\end_inset

 are independent, and 
\begin_inset Formula $X_{k}$
\end_inset

 is subexponential with parameters 
\begin_inset Formula $\left(\nu_{k},b_{k}\right),$
\end_inset

 and has mean 
\begin_inset Formula $\mu_{k}=\mathbb{E}\left[X_{k}\right]$
\end_inset

.
 The MGF is:
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}\left[\exp\left\{ \lambda\sum_{k=1}^{n}\left(X_{k}-\mu_{k}\right)\right\} \right]= & \prod_{k=1}^{n}\mathbb{E}\left[\exp\left\{ \lambda X_{k}\right\} \right]\\
\le & \prod_{i=1}\exp\left\{ \frac{\lambda^{2}\nu_{k}^{2}}{2}\right\} ,\forall\left|\lambda\right|<\left(\max_{k=1,...,n}b_{k}\right)^{-1}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $Z_{k}\overset{i.i.d}{\sim}\mathcal{N}\left(0,1\right).$
\end_inset

 Then we have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[\left|\frac{1}{n}\sum_{k=1}^{n}Z_{k}^{2}-1\right|\ge t\right]\le & 2\exp\left\{ -\frac{nt^{2}}{8}\right\} ,\forall t\in\left(0,1\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Theorem
Equivalent Characterizations of sub-exponential variables.
 For a zero-mean RV 
\begin_inset Formula $X,$
\end_inset

 the following statements are equivalent:
\end_layout

\begin_layout Theorem
\begin_inset Formula 
\begin{alignat*}{1}
(i). & \exists\nu>0,b>0:\mathbb{E}\left[\exp\left(\lambda X\right)\right]\le\exp\left\{ \frac{\nu^{2}\lambda^{2}}{2}\right\} ,\forall\left|\lambda\right|<\frac{1}{b}\\
(ii). & \exists c_{0}>0:\mathbb{E}\left[\exp\left(\lambda X\right)\right]<\infty,\forall\left|\lambda\right|\le c_{0}\\
(iii). & \exists c_{1},c_{2}>0:\mathbb{P}\left[\left|X\right|\ge t\right]\le c_{1}\exp\left(-c_{2}t\right),\forall t\ge0\\
(iv). & \gamma\coloneqq\sup_{k\ge2}\left[\frac{\mathbb{E}\left[X^{k}\right]}{k!}\right]^{1/k}<\infty
\end{alignat*}

\end_inset


\end_layout

\end_body
\end_document
