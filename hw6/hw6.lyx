#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
ST210A - Homework 6
\end_layout

\begin_layout Author
Hoang Duong
\end_layout

\begin_layout Problem
\begin_inset Formula $X_{1},...,X_{n}\stackrel{iid}{\sim}\mathcal{N}(\theta,\theta).$
\end_inset

 
\end_layout

\begin_layout Proof
We have:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
{\rm Var}(\bar{X}-\theta)= & \frac{1}{n}\theta\\
{\rm Var}(s^{2}-\theta)= & {\rm Var}(s^{2})=\mathbb{E}s^{4}-\mathbb{E}^{2}\left[s^{2}\right]\\
\mathbb{E}\left[s^{2}\right]= & \theta\\
\mathbb{E}\left[s^{4}\right]= & \mathbb{E}\left[\left(\frac{1}{n-1}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\right)\right]^{2}\\
= & \frac{1}{(n-1)^{2}}\mathbb{E}\left[\sum_{i=1}^{n}\left(\left(X_{i}-\theta\right)^{2}+\left(\theta-\bar{X}\right)^{2}+2\left(X_{i}-\theta\right)\left(\theta-\bar{X}\right)\right)\right]^{2}\\
= & \frac{1}{(n-1)^{2}}\mathbb{E}\left[\sum_{i=1}^{n}\left(X_{i}-\theta\right)^{2}-n\left(\theta-\bar{X}\right)^{2}\right]^{2}:=A
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Now let 
\begin_inset Formula $Y_{i}=X_{i}-\theta,$
\end_inset

 then 
\begin_inset Formula $Y_{i}\stackrel{iid}{\sim}\mathcal{N}(0,\theta)$
\end_inset

.
 Thus 
\begin_inset Formula $\mathbb{E}Y_{i}^{4}=3\theta^{2}.$
\end_inset

 So:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
A= & \frac{1}{(n-1)^{2}}\mathbb{E}\left[\sum_{i=1}^{n}Y_{i}^{2}-n\bar{Y}^{2}\right]^{2}\\
= & \frac{1}{(n-1)^{2}}\mathbb{E}\left[\left(\sum_{i=1}^{n}Y_{i}^{2}\right)^{2}-2n\bar{Y}^{2}\sum_{i=1}^{n}Y_{i}^{2}+n^{2}\bar{Y}^{4}\right]\\
\mathbb{E}\left(\sum_{i=1}^{n}Y_{i}^{2}\right)^{2}= & n\mathbb{E}Y_{1}^{4}+n(n-1)\mathbb{E}Y_{1}^{2}\mathbb{E}Y_{2}^{2}\\
= & 3n\theta^{2}+n(n-1)\theta^{2}=(n^{2}+2n)\theta^{2}\\
-2n\mathbb{E}\left[\bar{Y}^{2}\sum Y_{i}^{2}\right]= & \frac{-2}{n}\mathbb{E}\left[\left(\sum_{i=1}^{n}Y_{i}\right)^{2}\sum_{i=1}^{n}Y_{i}^{2}\right]\\
= & -\frac{2}{n}(n^{2}+2n)\theta^{2}\\
= & -2(n+2)\theta^{2}\\
n^{2}\mathbb{E}\bar{Y}^{4}= & \frac{1}{n^{2}}\mathbb{E}\left[\sum Y_{i}\right]^{4}\\
= & \frac{1}{n^{2}}3n^{2}\theta^{2}\\
= & 3\theta^{2}\\
\Rightarrow A= & \frac{1}{(n-1)^{2}}\theta^{2}\left(n^{2}+2n-2n-4+3\right)\\
= & \frac{n+1}{n-1}\theta^{2}\\
{\rm Var}\left[s^{2}\right]= & \mathbb{E}\left[s^{4}\right]-\mathbb{E}^{2}\left[s^{2}\right]\\
= & \frac{n+1}{n-1}\theta^{2}-\theta^{2}=\frac{2}{n-1}\theta^{2}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
So the relative efficiency of 
\begin_inset Formula $S^{2}$
\end_inset

 w.r.t.
 
\begin_inset Formula $\bar{X}$
\end_inset

 is:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\frac{{\rm Var}\bar{X}}{{\rm Var}S^{2}}= & \frac{\theta}{n}\frac{n-1}{2\theta^{2}}=\frac{n-1}{2n\theta}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Roughly speaking, with 
\begin_inset Formula $\theta>1/2,$
\end_inset

 the sample mean is more effective than the sample variance.
 With 
\begin_inset Formula $\theta<1/2,$
\end_inset

 the sample variance is more effective.
 
\end_layout

\begin_layout Problem
Exponential Distribution
\end_layout

\begin_layout Proof
1.
 Consider the CDF of 
\begin_inset Formula $X_{(2)},$
\end_inset

 
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[X_{(2)}\le x/n^{p}\right]= & 1-\mathbb{P}\left[X_{2}>x/n^{p}\right]\\
\mathbb{P}\left[X_{(2)}>x/n^{p}\right]= & \mathbb{P}\left[X_{(1)}>x/n^{p}\right]+\mathbb{P}\left[X_{(1)}\le x/n^{p}<X_{(2)}\right]\\
= & \mathbb{P}\left[X_{i}>x/n^{p},\forall i\right]+n\mathbb{P}\left[X_{1}\le x/n^{p}\right]\mathbb{P}\left[X_{i}>x/n^{p},\forall i\ge2\right]\\
= & \exp\left(-nx/n^{p}\right)+n\left(1-\exp\left(-x/n^{p}\right)\right)\exp\left\{ -(n-1)x/n^{p}\right\} \\
= & \frac{1+n\exp\left(x/n^{p}\right)-n}{\exp\left(nx/n^{p}\right)}:=\frac{A_{n}}{B_{n}}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
We note that 
\begin_inset Formula $\mathbb{P}\left[n^{p}X_{(2)}\le x\right]$
\end_inset

 converges to a value between 0 and 1 iff 
\begin_inset Formula $\mathbb{P}\left[n^{p}X_{(2)}>x\right]$
\end_inset

 converges to a value between 0 and 1.
 
\end_layout

\begin_layout Proof
Now if 
\begin_inset Formula $p>1,$
\end_inset

 using the fact that 
\begin_inset Formula $\lim_{x\rightarrow0}\frac{e^{x}-1}{x}=1,$
\end_inset

 we have:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\lim A_{n}= & 1+\lim n(\exp(x/n^{p})-1)\\
= & 1+\lim_{n\rightarrow\infty}\frac{\exp\left(x/n^{p}\right)-1}{\frac{x}{n^{p}}}\frac{x}{n^{p-1}}\\
= & 1+1\times0=1\\
\lim B_{n}= & 1\\
\Rightarrow\lim\frac{A_{n}}{B_{n}}= & 1
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
If 
\begin_inset Formula $0<p<1,$
\end_inset

 then:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\lim A_{n}= & 1+\lim_{n\rightarrow\infty}\frac{\exp\left(x/n^{p}\right)-1}{\frac{x}{n^{p}}}\frac{x}{n^{p-1}}=\infty\\
\lim B_{n}= & \infty\\
\lim\frac{A_{n}^{'}}{B_{n}^{'}}= & \lim\frac{\exp\left(x/n^{p}\right)-nxpn^{-p-1}\exp\left(x/n^{p}\right)-1}{(1-p)n^{-p}x\exp\left(n^{1-p}x)\right)}\\
\lim\frac{\partial A_{n}}{\partial n}= & \lim\left(\exp\left(x/n^{p}\right)-npxn^{-p-1}\exp\left(x/n^{p}\right)-1\right)\\
= & \lim\left(\exp(x/n^{p})-px\frac{\exp(xn^{-p})}{n^{p}}-1\right)\\
= & 1-0-1=0\\
\lim\frac{\partial B_{n}}{\partial n}= & \lim\frac{(1-p)x\exp\left(n^{1-p}x\right)}{n^{p}}=\infty.\text{ (\exp\ \rightarrow\infty\ faster than polynomial)}\\
\Rightarrow\lim\frac{A_{n}}{B_{n}}= & 0\text{ (L'Hospital's Rule)}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Finally for 
\begin_inset Formula $p=1,$
\end_inset

 then:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\lim\frac{A_{n}}{B_{n}}= & \lim\frac{1+n\exp(x/n)-n}{\exp(x)}\\
= & \lim\exp(-x)\left(1+\frac{\exp(x/n)-1}{\frac{1}{n}}\right)\\
= & \exp(-x)(1+x)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
So 
\begin_inset Formula $p=1.$
\end_inset

 With this choice of 
\begin_inset Formula $p,$
\end_inset

 
\begin_inset Formula $\mathbb{P}\left[X_{(2)}n\le x\right]=1-\frac{1+x}{\exp(x)}$
\end_inset

 
\end_layout

\begin_layout Proof
2.
 We have:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{P}\left[X_{(n)}-\log n\le x\right]= & \mathbb{P}\left[X_{(n)}\le x+\log n\right]\\
= & \mathbb{P}\left[X_{i}\le x+\log n,\forall i\right]\\
= & \mathbb{P}^{n}\left[X_{1}\le x+\log n\right]\\
= & \left(1-\exp\left(-x-\log n\right)\right)^{n}\\
= & \left(1-\exp(-x)/n\right)^{n}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
We use the limit equality that: 
\begin_inset Formula $\lim\left(1+\frac{a}{n}\right)^{n}=\exp(a),$
\end_inset

 then:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\lim\mathbb{P}\left[X_{(n)}-\log n\le x\right]= & \lim\left(1+\frac{-\exp(-x)}{n}\right)^{n}=\exp\left(-\exp(-x)\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
So the limiting CDF for 
\begin_inset Formula $X_{(n)}-\log n$
\end_inset

 is 
\begin_inset Formula $\exp\left(-\exp(-x)\right)$
\end_inset

.
\end_layout

\begin_layout Lemma
Leibniz's Rule
\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $\phi(\alpha)=\int_{u_{1}(\alpha)}^{u_{2}(\alpha)}f(x,\alpha)dx,\alpha\in\left[a,b\right]$
\end_inset

 then:
\end_layout

\begin_layout Lemma
\begin_inset Formula 
\begin{alignat*}{1}
\frac{d\phi}{d\alpha} & =\int_{u_{1}}^{u_{2}}\frac{\partial f}{\partial\alpha}dx+f(u_{2},\alpha)\frac{du_{2}}{d\alpha}-f(u_{1},\alpha)\frac{du_{1}}{d\alpha}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Lemma
for 
\begin_inset Formula $\alpha\in\left[a,b\right]$
\end_inset

.
 
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Lemma
Let X be a random variable with CDF F.
 Let 
\begin_inset Formula $k_{1},k_{2}\in\mathbb{R}^{+}$
\end_inset

.
 Define:
\end_layout

\begin_layout Lemma
\begin_inset Formula 
\begin{alignat*}{1}
L(X,a)= & \begin{cases}
k_{1}\left|X-a\right|, & a\le X\\
k_{2}\left|X-a\right|, & a>X
\end{cases}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Lemma
Then a that minimizes 
\begin_inset Formula $\mathbb{E}L(X,a)$
\end_inset

 is the 
\begin_inset Formula $F^{-1}(\frac{k_{2}}{k_{1}+k_{2}}).$
\end_inset

 
\end_layout

\begin_layout Proof
We have:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}L(X,a)= & \int_{-\infty}^{a}k_{1}(a-x)dF+\int_{a}^{\infty}k_{2}(x-a)dF\\
\frac{d\mathbb{E}L(X,a)}{da}= & \int_{-\infty}^{a}k_{1}dF+k_{1}(a-a)+\int_{a}^{\infty}-k_{2}dF-k_{2}(a-a)\\
= & \int_{-\infty}^{a}k_{1}dF-\int_{a}^{\infty}k_{2}dF\\
\frac{d\mathbb{E}L(X,a)}{da}= & 0\\
\Leftrightarrow\int_{-\infty}^{a}k_{1}dF= & \int_{a}^{\infty}k_{2}dF\\
\Leftrightarrow k_{1}F(a)= & k_{2}(1-F(a))\\
\Leftrightarrow F(a)= & \frac{k_{2}}{k_{1}+k_{2}}\\
\Leftrightarrow a= & F^{-1}\left(\frac{k_{2}}{k_{1}+k_{2}}\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
To verify that we have the minimum at the point where first derivative is
 equal to zero, we need to check the second derivative:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\frac{d^{2}\mathbb{E}L(X,a)}{da^{2}}= & k_{1}+k_{2}>0.
\end{alignat*}

\end_inset

 So 
\begin_inset Formula $\mathbb{E}L(X,a)$
\end_inset

 is minimized when 
\begin_inset Formula $a=F^{-1}\left(\frac{k_{2}}{k_{1}+k_{2}}\right).$
\end_inset

 
\end_layout

\begin_layout Problem
Loss Function that is minimized at quantile
\end_layout

\begin_layout Proof
Applying Lemma 2 we just proved, and Theorem 7.1 about Bayes estimator in
 Keener's book, we have the Bayes estimator is the 
\begin_inset Formula $\frac{k_{2}}{k_{1}+k_{2}}$
\end_inset

 quantile of the posterior distribution.
 
\end_layout

\begin_layout Problem
Normal Distribution
\end_layout

\begin_layout Proof
For i.i.d random variable, the aggregate log likelihood function is just sum
 of individual likelihood function.
 Thus:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
-l(\theta)=-\log\mathcal{L}(\theta)= & \sum_{i=1}^{n}\sum_{j=1}^{r}\left(\log\sigma+\frac{1}{2\sigma^{2}}\left(X_{ij}-\mu_{i}\right)^{2}\right)+C
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Looking at 
\begin_inset Formula $\mu_{i},$
\end_inset

 the negative log-likelihood function is minimized when 
\begin_inset Formula $\mu_{i}=\bar{X}_{i}.$
\end_inset

 So now we need to find the 
\begin_inset Formula $\sigma$
\end_inset

 that minimize:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
-l(\theta)= & \sum_{i=1}^{n}\sum_{j=1}^{r}\left(\log\sigma+\frac{1}{2\sigma^{2}}\left(X_{ij}-\bar{X}_{i}\right)^{2}\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Looking at the derivative:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\frac{\partial-l(\theta)}{\partial\sigma}= & \frac{nr}{\sigma}-\frac{1}{\sigma^{3}}\sum_{i=1}^{n}\sum_{j=1}^{r}\left(X_{ij}-\bar{X}_{i}\right)^{2}\\
\frac{\partial-l(\theta)}{\partial\sigma}= & 0\\
\Leftrightarrow\sigma^{2}= & \frac{1}{nr}\sum_{i=1}^{n}\sum_{j=1}^{r}\left(X_{ij}-\bar{X}_{i}\right)^{2}:=\sigma_{MLE}^{2}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
To make sure that 
\begin_inset Formula $\sigma_{MLE}^{2}$
\end_inset

 is the minimizer, we check the second derivative, which is:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\frac{\partial^{2}-l(\theta)}{\partial\sigma^{2}}= & -\frac{nr}{\sigma^{2}}+\frac{3}{\sigma^{4}}\sum_{i=1}^{n}\sum_{j=1}^{r}\left(X_{ij}-\bar{X}_{i}\right)^{2}\ge0\\
\Leftrightarrow\sigma^{2}\le & \frac{3}{nr}\sum_{i=1}^{n}\sum_{j=1}^{r}\left(X_{ij}-\bar{X}_{i}\right)^{2}\text{ (*)}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
The last statement, (*), is not always true.
 However since 
\begin_inset Formula $\mathbb{E}\sum_{i=1}^{n}\sum_{j=1}^{r}\left(X_{ij}-\bar{X}_{i}\right)^{2}=n(r-1)\sigma^{2},$
\end_inset

 we can safely say that it is true with high probability.
 Assuming this then the MLE estimator for 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is 
\begin_inset Formula $\frac{1}{nr}\sum_{i=1}^{n}\sum_{j=1}^{r}\left(X_{ij}-\bar{X}_{i}\right)^{2}.$
\end_inset

 Now:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\mathbb{E}\frac{1}{nr}\sum_{i=1}^{n}\sum_{j=1}^{r}\left(X_{ij}-\bar{X}_{i}\right)^{2}= & \frac{n(r-1)}{nr}\sigma^{2}=\frac{r-1}{r}\sigma^{2}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
So 
\begin_inset Formula $\lim_{n\rightarrow\infty}\sigma_{MLE}^{2}=\frac{r-1}{r}\sigma^{2}\neq\sigma^{2}.$
\end_inset

 Thus the MLE estimator for 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is not consistent.
 
\end_layout

\begin_layout Problem
Mixture Distribution
\end_layout

\begin_layout Proof
(a) Again we observe that log likelihood function of i.i.d random variables
 is the sum of individual log likelihood function.
 Thus:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
l(\theta)= & \sum_{i=1}^{n}\log\left(\theta f_{1}(x_{i})+(1-\theta)f_{2}(x_{i})\right)\\
\Rightarrow\frac{\partial l(\theta)}{\partial\theta}= & \sum_{i=1}^{n}\frac{f_{1}(x_{i})-f_{2}(x_{i})}{\theta f_{1}(x_{i})+(1-\theta)f_{2}(x_{i})}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Now we note that for any linear function 
\begin_inset Formula $g(\theta)$
\end_inset

 of 
\begin_inset Formula $\theta.$
\end_inset

 The second derivative:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\frac{\partial\log g(\theta)}{\partial\theta}= & \frac{g'(\theta)}{g(\theta)}\\
\Rightarrow\frac{\partial^{2}\log g(\theta)}{\partial\theta^{2}}= & \frac{g''(\theta)g(\theta)-\left(g'(\theta)\right)^{2}}{g^{2}(\theta)}=-\frac{\left(g'(\theta)\right)^{2}}{g^{2}(\theta)}\le0
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
In our case the sum of n such second derivative is also non-positive.
 It is zero iff 
\begin_inset Formula $f_{1}(x_{i})=f_{2}(x_{i}),\forall i$
\end_inset

.
 We will make the weak assumption that this does not happen (otherwise the
 the solution for MLE might not be unique).
 With this assumption, the first derivative of our log likelihood is a decreasin
g function on 
\begin_inset Formula $(0,1).$
\end_inset

 Thus it has a unique root iff 
\begin_inset Formula $\lim_{\theta\rightarrow0^{+}}\frac{\partial l(\theta)}{\partial\theta}>0$
\end_inset

 and 
\begin_inset Formula $\lim_{\theta\rightarrow1^{-}}\frac{\partial l(\theta)}{\partial\theta}<0$
\end_inset

.
 The first condition is equivalent to:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\sum_{i=1}^{n}\frac{f_{1}(x_{i})-f_{2}(x_{i})}{f_{2}(x_{i})}> & 0\\
\Leftrightarrow\sum_{i=1}^{n}\frac{f_{1}(x_{i})}{f_{2}(x_{i})}> & n
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
While the second condition is equivalent to:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\sum_{i=1}^{n}\frac{f_{1}(x_{i})-f_{2}(x_{i})}{f_{1}(x_{i})}< & 0\\
\Leftrightarrow\sum_{i=1}^{n}\frac{f_{2}(x_{i})}{f_{1}(x_{i})}> & n
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Since we've already shown that the second derivative is negative, the solution
 to the first derivative equal to zero is the maximizer of the function.
 
\end_layout

\begin_layout Proof
(b) When the score equation has no solution, there are two cases.
 
\end_layout

\begin_layout Proof
First case, the first derivative of the log likelihood is always bigger
 than zero, then the log likelihood is maximized as 
\begin_inset Formula $\theta\rightarrow1.$
\end_inset

 The MLE for 
\begin_inset Formula $\theta$
\end_inset

 is 1.
 
\end_layout

\begin_layout Proof
Second case, the first derivative of the log likelihood is always smaller
 than zero, then the log likelihood is maximized as 
\begin_inset Formula $\theta\rightarrow0$
\end_inset

.
 The MLE for 
\begin_inset Formula $\theta$
\end_inset

 is 0.
 
\end_layout

\end_body
\end_document
